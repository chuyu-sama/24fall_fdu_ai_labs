{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1 PyTorch Tensor基础\n",
    "在这个notebook中，我们将学习PyTorch Tensor的一些基础知识，并完成最后的练习。\n",
    "\n",
    "同学可以阅读教程并逐个运行代码块来观察程序输出。Lab 1.1中不需要用到大量的GPU加速，只需要同学们在notebook最后部分尝试。如果要打开GPU加速，请参考Lab1 PPT中阿里云算力使用的说明。\n",
    "\n",
    "如果是CPU环境，下面的torch.cuda.is_available()会输出False；GPU环境则输出True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-18T11:36:05.198967Z",
     "iopub.status.busy": "2024-10-18T11:36:05.198614Z",
     "iopub.status.idle": "2024-10-18T11:36:05.202078Z",
     "shell.execute_reply": "2024-10-18T11:36:05.201556Z",
     "shell.execute_reply.started": "2024-10-18T11:36:05.198949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么需要张量？\n",
    "\n",
    "为了回答这个问题，我们将PyTorch张量和Python列表、NumPy数组做了一个简单的对比。同时我们比较使用GPU加速的PyTorch张量计算和仅用CPU计算的情况。下面这段代码同学们可以不用运行，结果已经给出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-10-19T09:15:17.382150Z",
     "iopub.status.busy": "2024-10-19T09:15:17.381582Z",
     "iopub.status.idle": "2024-10-19T09:15:17.459657Z",
     "shell.execute_reply": "2024-10-19T09:15:17.459060Z",
     "shell.execute_reply.started": "2024-10-19T09:15:17.382131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python List Time: 0.045069217681884766 seconds\n",
      "NumPy Array Time: 0.0017247200012207031 seconds\n",
      "PyTorch Tensor Time: 0.0009222030639648438 seconds\n",
      "PyTorch Tensor Cuda Time: 0.00013303756713867188 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "how_long = 1000000\n",
    "\n",
    "a = list(range(how_long))\n",
    "b = np.arange(how_long)\n",
    "c = torch.arange(how_long)\n",
    "d = torch.arange(how_long).cuda()\n",
    "\n",
    "start_time = time.time() #记录当前时间戳\n",
    "a = [tmp + 1 for tmp in a] #这是一个列表推导式 操作/取出元素/对象\n",
    "print(f\"Python List Time: {time.time() - start_time} seconds\") \n",
    "#f为格式化字符串，允许在字符串里插入变量，大括号包裹即可\n",
    "\n",
    "start_time = time.time()\n",
    "b = b + 1.0 #对 b 中的所有元素逐个执行加法运算,直接作用在整个数组\n",
    "print(f\"NumPy Array Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "c = c + 1.0\n",
    "print(f\"PyTorch Tensor Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "d = d + 1.0\n",
    "print(f\"PyTorch Tensor Cuda Time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "attachments": {
    "4f74880d-7355-42af-ae01-2363bc0e9506.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对一个长度为一百万的数组每个元素加一，我们可以看到四种方式的用时比较：\n",
    "\n",
    "![1708170565359.png](attachment:4f74880d-7355-42af-ae01-2363bc0e9506.png)\n",
    "\n",
    "速度上：PyTorch Tensor Cuda (GPU) > PyTorch Tensor (CPU) > NumPy Array > Python List\n",
    "\n",
    "利用GPU并行计算的特性，速度会大大加快。深度学习中涉及大量的矩阵加法、矩阵乘法，而这些操作可以很好地并行，因此使用PyTorch Tensor，可以帮助我们高效地进行深度学习的模型训练、推理。尤其是在深度学习模型、数据集规模都非常庞大的今天。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor基础\n",
    "\n",
    "### 创建、读写张量\n",
    "+ 一个张量（Tensor）是一个多维的数组，其中所有元素值的类型（int, long, boolean, float32, ...）相同。\n",
    "+ 张量维度的数量是张量的秩（rank）；张量的形状（shape）是一个整数元组，给出了数组沿每个维度的大小。\n",
    "+ 我们可以从Python列表、NumPy数组来初始化torch张量。类似于Python列表，我们可以使用方括号访问或修改PyTorch张量的元素。\n",
    "+ 从PyTorch张量访问一个元素返回一个PyTorch标量；我们可以使用.item()方法将其转换为Python标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-18T07:27:20.005585Z",
     "iopub.status.busy": "2024-10-18T07:27:20.004950Z",
     "iopub.status.idle": "2024-10-18T07:27:20.021865Z",
     "shell.execute_reply": "2024-10-18T07:27:20.021183Z",
     "shell.execute_reply.started": "2024-10-18T07:27:20.005559Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is b:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 5]])\n",
      "rank of b: 2\n",
      "b.shape:  torch.Size([2, 3])\n",
      "\n",
      "b[0, 1]: tensor(2)\n",
      "b[1, 2]: tensor(5)\n",
      "\n",
      "b after mutating:\n",
      "tensor([[  1,   2,   3],\n",
      "        [  4, 100,   5]])\n"
     ]
    }
   ],
   "source": [
    "# Create a two-dimensional tensor\n",
    "import torch\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 5]])\n",
    "print('Here is b:')\n",
    "print(b)\n",
    "print('rank of b:', b.dim())\n",
    "print('b.shape: ', b.shape)\n",
    "# Access elements\n",
    "print()\n",
    "print('b[0, 1]:', b[0, 1]) #第0行第0列开始\n",
    "print('b[1, 2]:', b[1, 2])\n",
    "# Mutate elements\n",
    "b[1, 1] = 100\n",
    "print()\n",
    "print('b after mutating:')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tensor构造函数\n",
    "PyTorch提供了很多内置的构造函数来构造张量，从而避免我们重复地手写Python列表。一些常用的构造函数如下：\n",
    "+ [`torch.zeros`](https://pytorch.org/docs/stable/generated/torch.zeros.html): 创建一个全是0的张量\n",
    "+ [`torch.ones`](https://pytorch.org/docs/stable/generated/torch.ones.html): 创建一个全是1的张量\n",
    "+ [`torch.rand`](https://pytorch.org/docs/stable/generated/torch.rand.html): 创建一个包含正态分布随机数的张量\n",
    "+ [更多构造函数](https://pytorch.org/docs/stable/torch.html#creation-ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T11:42:52.575088Z",
     "iopub.status.busy": "2024-10-17T11:42:52.574527Z",
     "iopub.status.idle": "2024-10-17T11:42:52.580366Z",
     "shell.execute_reply": "2024-10-17T11:42:52.579895Z",
     "shell.execute_reply.started": "2024-10-17T11:42:52.575069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor of zeros:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "tensor of ones:\n",
      "tensor([[1., 1.]])\n",
      "\n",
      "identity matrix:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "random tensor:\n",
      "tensor([[0.9664, 0.2850, 0.4516, 0.8149, 0.1477],\n",
      "        [0.3619, 0.9907, 0.1075, 0.1545, 0.5399],\n",
      "        [0.1312, 0.2888, 0.8977, 0.6411, 0.9614],\n",
      "        [0.5705, 0.1212, 0.0764, 0.3816, 0.3106]])\n",
      "shape of d: torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of all zeros\n",
    "a = torch.zeros(2, 3)\n",
    "print('tensor of zeros:')\n",
    "print(a)\n",
    "\n",
    "# Create a tensor of all ones\n",
    "b = torch.ones(1, 2)\n",
    "print('\\ntensor of ones:')\n",
    "print(b)\n",
    "\n",
    "# Create a 3x3 identity matrix\n",
    "c = torch.eye(3)\n",
    "print('\\nidentity matrix:')\n",
    "print(c)\n",
    "\n",
    "# Tensor of random values\n",
    "d = torch.rand(4, 5)\n",
    "print('\\nrandom tensor:')\n",
    "print(d)\n",
    "print(f'shape of d: {d.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据类型\n",
    "在上面的例子中，你可能已经注意到，我们的一些张量包含了浮点值，而其他的包含了整数值。PyTorch 提供了一大套数值数据类型，你可以用它们来构造张量。当你创建一个张量时，PyTorch 会尝试猜测一个数据类型；构造张量的函数通常有一个 dtype 参数，你可以使用它来显式指定一个数据类型，也可以对某个张量访问这个参数来知道它的数据类型是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T11:42:55.403610Z",
     "iopub.status.busy": "2024-10-17T11:42:55.403043Z",
     "iopub.status.idle": "2024-10-17T11:42:55.409033Z",
     "shell.execute_reply": "2024-10-17T11:42:55.408537Z",
     "shell.execute_reply.started": "2024-10-17T11:42:55.403591Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype when torch chooses for us:\n",
      "List of integers: torch.int64\n",
      "List of floats: torch.float32\n",
      "Mixed list: torch.float32\n",
      "\n",
      "dtype when we force a datatype:\n",
      "32-bit float:  torch.float32\n",
      "32-bit integer:  torch.int32\n",
      "\n",
      "torch.ones with different dtypes\n",
      "default dtype: torch.float32\n",
      "16-bit integer: torch.int16\n"
     ]
    }
   ],
   "source": [
    "# Let torch choose the datatype\n",
    "x0 = torch.tensor([1, 2])   # List of integers\n",
    "x1 = torch.tensor([1., 2.]) # List of floats\n",
    "x2 = torch.tensor([1., 2])  # Mixed list\n",
    "print('dtype when torch chooses for us:')\n",
    "print('List of integers:', x0.dtype)\n",
    "print('List of floats:', x1.dtype)\n",
    "print('Mixed list:', x2.dtype)\n",
    "\n",
    "# Force a particular datatype\n",
    "y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n",
    "y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit (signed) integer\n",
    "print('\\ndtype when we force a datatype:')\n",
    "print('32-bit float: ', y0.dtype)\n",
    "print('32-bit integer: ', y1.dtype)\n",
    "\n",
    "# Other creation ops also take a dtype argument\n",
    "z0 = torch.ones(1, 2)  # Let torch choose for us\n",
    "z1 = torch.ones(1, 2, dtype=torch.int16) # 16-bit (signed) integer\n",
    "print('\\ntorch.ones with different dtypes')\n",
    "print('default dtype:', z0.dtype)\n",
    "print('16-bit integer:', z1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor索引（切片，Slice）\n",
    "类似于 Python 列表和 numpy 数组，PyTorch 张量可以使用 start:stop 或 start:stop:step 语法进行切片。这里和Python列表类似是左闭右开区间，即选择出来的元素包含start，但是不包含stop。\n",
    "\n",
    "开始和停止索引可以是负数，在这种情况下，它们从张量的末尾向后计数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T11:42:58.856113Z",
     "iopub.status.busy": "2024-10-17T11:42:58.855647Z",
     "iopub.status.idle": "2024-10-17T11:42:58.864073Z",
     "shell.execute_reply": "2024-10-17T11:42:58.863617Z",
     "shell.execute_reply.started": "2024-10-17T11:42:58.856095Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "1 [3, 4, 5]\n",
      "2 [3, 4, 5, 6, 7, 8, 9, 10]\n",
      "3 [1, 2, 3, 4, 5]\n",
      "4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "5 [2, 4]\n",
      "6 [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "7 [7, 9]\n",
      "0 tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "1 tensor([3, 4, 5])\n",
      "2 tensor([ 3,  4,  5,  6,  7,  8,  9, 10])\n",
      "3 tensor([1, 2, 3, 4, 5])\n",
      "4 tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "5 tensor([2, 4])\n",
      "6 tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "7 tensor([7, 9])\n"
     ]
    }
   ],
   "source": [
    "# A quick review for slicing Python list\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(0, a)        # (0) Original\n",
    "print(1, a[2:5])   # (1) Elements between index 2 and 5\n",
    "print(2, a[2:])    # (2) Elements after index 2\n",
    "print(3, a[:5])    # (3) Elements before index 5\n",
    "print(4, a[:])     # (4) All elements\n",
    "print(5, a[1:5:2]) # (5) Every second element between indices 1 and 5\n",
    "print(6, a[:-1])   # (6) All but the last element\n",
    "print(7, a[-4::2]) # (7) Every second element, starting from the fourth-last\n",
    "\n",
    "# PyTorch has the same behavior\n",
    "a = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(0, a)        # (0) Original tensor\n",
    "print(1, a[2:5])   # (1) Elements between index 2 and 5\n",
    "print(2, a[2:])    # (2) Elements after index 2\n",
    "print(3, a[:5])    # (3) Elements before index 5\n",
    "print(4, a[:])     # (4) All elements\n",
    "print(5, a[1:5:2]) # (5) Every second element between indices 1 and 5\n",
    "print(6, a[:-1])   # (6) All but the last element\n",
    "print(7, a[-4::2]) # (7) Every second element, starting from the fourth-last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多维数组，你可以为每一个数组维度提供这样一个切片来索引出你需要的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-17T11:43:05.728318Z",
     "iopub.status.busy": "2024-10-17T11:43:05.727741Z",
     "iopub.status.idle": "2024-10-17T11:43:05.733188Z",
     "shell.execute_reply": "2024-10-17T11:43:05.732724Z",
     "shell.execute_reply.started": "2024-10-17T11:43:05.728299Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "tensor([[ 5,  6],\n",
      "        [ 9, 10]]) torch.Size([2, 2])\n",
      "tensor([[5]]) torch.Size([1, 1])\n",
      "tensor(5) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Create the following rank 2 tensor with shape (3, 4)\n",
    "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print('Original tensor')\n",
    "print(a)\n",
    "\n",
    "# From row 1 to row 2; From col 0 to col 1\n",
    "a1 = a[1:3, 0:2]\n",
    "print(a1, a1.shape)\n",
    "\n",
    "# From row 1 to row 1; From col 0 to col 0\n",
    "a2 = a[1:2, 0:1]\n",
    "print(a2, a2.shape) # rank is preserved\n",
    "\n",
    "# row 1; col 0\n",
    "\n",
    "a3 = a[1, 0]\n",
    "print(a3, a3.shape) # rank is not preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor索引（数组索引）\n",
    "当你使用切片对 torch 张量进行索引时，得到的张量视图将始终是原张量的一个子数组。\n",
    "相比于切片，我们还可以使用一个数组来索引张量，让我们在构造新张量时比使用切片拥有更多的灵活性。\n",
    "\n",
    "这个步骤可以理解为：我们先定义一个数组，并将数组中的每一个元素作为索引找出我们需要的部分，并按照数组的顺序将这些部分组合成新的张量。\n",
    "\n",
    "例如，我们可以使用数组来按照某一个特定维度索引张量（一次取出某一个行/列），也可以同时对多个维度进行索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-17T11:43:23.164656Z",
     "iopub.status.busy": "2024-10-17T11:43:23.164190Z",
     "iopub.status.idle": "2024-10-17T11:43:23.170641Z",
     "shell.execute_reply": "2024-10-17T11:43:23.170163Z",
     "shell.execute_reply.started": "2024-10-17T11:43:23.164638Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "\n",
      "Reordered rows:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 1,  2,  3,  4],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 5,  6,  7,  8]])\n",
      "\n",
      "Reordered columns:\n",
      "tensor([[ 4,  3,  2,  1],\n",
      "        [ 8,  7,  6,  5],\n",
      "        [12, 11, 10,  9]])\n",
      "\n",
      "Indexing multiple dimensions:\n",
      "tensor([10,  7,  1])\n"
     ]
    }
   ],
   "source": [
    "# Create the following rank 2 tensor with shape (3, 4)\n",
    "# [[ 1  2  3  4]\n",
    "#  [ 5  6  7  8]\n",
    "#  [ 9 10 11 12]]\n",
    "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print('Original tensor:')\n",
    "print(a)\n",
    "\n",
    "# Create a new tensor of shape (5, 4) by reordering rows from a:\n",
    "# - First two rows same as the first row of a\n",
    "# - Third row is the same as the last row of a\n",
    "# - Fourth and fifth rows are the same as the second row from a\n",
    "idx = [0, 0, 2, 1, 1]  # index arrays can be Python lists of integers\n",
    "print('\\nReordered rows:')\n",
    "print(a[idx])\n",
    "\n",
    "# Create a new tensor of shape (3, 4) by reversing the columns from a\n",
    "idx = torch.tensor([3, 2, 1, 0])  # Index arrays can be int64 torch tensors\n",
    "print('\\nReordered columns:')\n",
    "print(a[:, idx])\n",
    "\n",
    "# Indexing multiple dimensions together\n",
    "# idx_row and idx_col should have the same shape\n",
    "# or be broadcastable together (refer to following chapters for broadcasting)\n",
    "# In the following example, we index elements at (2, 1), (1, 2), (0, 0)\n",
    "idx_row = torch.tensor([2, 1, 0])\n",
    "idx_col = torch.tensor([1, 2, 0])\n",
    "print('\\nIndexing multiple dimensions:')\n",
    "print(a[idx_row, idx_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据索引给Tensor赋值\n",
    "当你使用切片、数组这两种方式来索引一个Tensor时，可以对选定的部分赋上新的值。这种赋值操作被称为in-place操作，也就是该张量并没有被新建，只有它内部的元素值发生变化。具体来说，它内存的地址没有变化。\n",
    "\n",
    "因此，如果我们把同一个张量赋值给多个Python变量，更改其中一个会导致原始张量发生变化。但是如果我们将某个张量克隆，它们就有了新的地址，可以被区分。我们看看代码区分这两种操作。\n",
    "\n",
    "我们通常可以用.data_ptr()来看两个变量是否指向了同一个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T11:44:39.672975Z",
     "iopub.status.busy": "2024-10-17T11:44:39.672367Z",
     "iopub.status.idle": "2024-10-17T11:44:39.680809Z",
     "shell.execute_reply": "2024-10-17T11:44:39.680259Z",
     "shell.execute_reply.started": "2024-10-17T11:44:39.672956Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "\n",
      "a with new Values at col.1:\n",
      "tensor([[  1, 100,   3,   4],\n",
      "        [  5, 100,   7,   8],\n",
      "        [  9, 100,  11,  12]])\n",
      "\n",
      "modifying b also changes a:\n",
      "tensor([[  1, 100,  10,   4],\n",
      "        [  5, 100,  10,   8],\n",
      "        [  9, 100,  10,  12]])\n",
      "tensor([[  1, 100,  10,   4],\n",
      "        [  5, 100,  10,   8],\n",
      "        [  9, 100,  10,  12]])\n",
      "\n",
      "modifying c does not change a or b:\n",
      "tensor([[  1, 100,  10,   4],\n",
      "        [  5, 100,  10,   8],\n",
      "        [  9, 100,  10,  12]])\n",
      "tensor([[  1, 100,  10,   4],\n",
      "        [  5, 100,  10,   8],\n",
      "        [  9, 100,  10,  12]])\n",
      "tensor([[  1, 100,  10,   0],\n",
      "        [  5, 100,  10,   0],\n",
      "        [  9, 100,  10,   0]])\n",
      "\n",
      "a's address':\n",
      "1155343296\n",
      "b's address':\n",
      "1155343296\n",
      "c's address':\n",
      "1155348160\n"
     ]
    }
   ],
   "source": [
    "# Create the following rank 2 tensor with shape (3, 4)\n",
    "# [[ 1  2  3  4]\n",
    "#  [ 5  6  7  8]\n",
    "#  [ 9 10 11 12]]\n",
    "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print('Original tensor:')\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# inplace operation: give new values to a based on indexing\n",
    "# given new values to col.1 \n",
    "a[:, 1] = 100\n",
    "print('a with new Values at col.1:')\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# create a tensor based on a\n",
    "b = a\n",
    "b[:, 2] = 10\n",
    "print('modifying b also changes a:')\n",
    "print(a)\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# create a tensor based on a's clone\n",
    "c = a.clone()\n",
    "c[:, 3] = 0\n",
    "print('modifying c does not change a or b:')\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "# a and b share the same memory addr\n",
    "print('a\\'s address\\':')\n",
    "print(a.data_ptr())\n",
    "print('b\\'s address\\':')\n",
    "print(b.data_ptr())\n",
    "print('c\\'s address\\':')\n",
    "print(c.data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形状操作\n",
    "#### View\n",
    "PyTorch 提供了许多方法来操作张量的形状。最简单的例子是 [`.view()`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)：这会返回一个新的张量，其元素数量与输入相同，但形状不同。\n",
    "我们可以使用 .view() 将矩阵展平成向量，以及将秩为 1 的向量转换成秩为 2 的行或列矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T12:05:58.469447Z",
     "iopub.status.busy": "2024-10-17T12:05:58.468844Z",
     "iopub.status.idle": "2024-10-17T12:05:58.476297Z",
     "shell.execute_reply": "2024-10-17T12:05:58.475706Z",
     "shell.execute_reply.started": "2024-10-17T12:05:58.469428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor x0:\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "shape: torch.Size([2, 4])\n",
      "\n",
      "Flattened tensor x1:\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "shape: torch.Size([8])\n",
      "\n",
      "Row vector x2:\n",
      "tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
      "shape: torch.Size([1, 8])\n",
      "\n",
      "Column vector x3:\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8]])\n",
      "shape: torch.Size([8, 1])\n",
      "\n",
      "Rank 3 tensor x4:\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "shape: torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print('Original tensor x0:')\n",
    "print(x0)\n",
    "print('shape:', x0.shape)\n",
    "\n",
    "# Flatten x0 into a rank 1 vector of shape (8,)\n",
    "x1 = x0.view(8)\n",
    "print('\\nFlattened tensor x1:')\n",
    "print(x1)\n",
    "print('shape:', x1.shape)\n",
    "\n",
    "# Convert x1 to a rank 2 \"row vector\" of shape (1, 8)\n",
    "x2 = x1.view(1, 8)\n",
    "print('\\nRow vector x2:')\n",
    "print(x2)\n",
    "print('shape:', x2.shape)\n",
    "\n",
    "# Convert x1 to a rank 2 \"column vector\" of shape (8, 1)\n",
    "x3 = x1.view(8, 1)\n",
    "print('\\nColumn vector x3:')\n",
    "print(x3)\n",
    "print('shape:', x3.shape)\n",
    "\n",
    "# Convert x1 to a rank 3 tensor of shape (2, 2, 2):\n",
    "x4 = x1.view(2, 2, 2)\n",
    "print('\\nRank 3 tensor x4:')\n",
    "print(x4)\n",
    "print('shape:', x4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便起见，调用 .view() 时可以包含一个单独的 -1 参数，-1代表了我们在该维度上放置尽可能多的元素。\n",
    "\n",
    "假设我们对于元素个数为 N=x*y*z 的张量做\n",
    "+ .view(x, -1, z) 操作，那么 -1 这个维度上会有 y 个元素\n",
    "+ .view(x, -1) 操作，那么 -1 这个维度上会有 yz 个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T12:07:36.875916Z",
     "iopub.status.busy": "2024-10-17T12:07:36.875314Z",
     "iopub.status.idle": "2024-10-17T12:07:36.880811Z",
     "shell.execute_reply": "2024-10-17T12:07:36.880201Z",
     "shell.execute_reply.started": "2024-10-17T12:07:36.875897Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "x0_flat:\n",
      "tensor([1, 2, 3, 4, 5, 6]) torch.Size([6])\n",
      "x0_flat:\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# We can reuse these functions for tensors of different shapes\n",
    "x0 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print('x0:')\n",
    "print(x0)\n",
    "print('x0_flat:')\n",
    "print(x0.view(-1), x0.view(-1).shape)\n",
    "print('x0_flat:')\n",
    "print(x0.view(1, -1), x0.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转置\n",
    "要执行的另一个常见的形状是转置矩阵。如果你尝试使用 .view() 转置矩阵，不可行：view() 函数按行主序取元素，因此你不能使用 .view() 来转置矩阵。.view() 只能用来给张量添加新的维度，或者折叠张量的相邻维度。\n",
    "\n",
    "对于其他类型的重塑操作，你通常需要使用可以交换张量轴的函数。最简单的此类函数是 .t()，专门用于转置矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T12:12:42.673311Z",
     "iopub.status.busy": "2024-10-17T12:12:42.672711Z",
     "iopub.status.idle": "2024-10-17T12:12:42.678772Z",
     "shell.execute_reply": "2024-10-17T12:12:42.678292Z",
     "shell.execute_reply.started": "2024-10-17T12:12:42.673291Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Transposing with view DOES NOT WORK!\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "Transposed matrix:\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print('Original matrix:')\n",
    "print(x)\n",
    "print('\\nTransposing with view DOES NOT WORK!')\n",
    "print(x.view(3, 2))\n",
    "print('\\nTransposed matrix:')\n",
    "print(torch.t(x))\n",
    "print(x.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".t() 方法只能用以处理两个维度的情况，如果出现了更多的维度，那么需要使用 [`torch.transpose`](https://pytorch.org/docs/stable/generated/torch.transpose.html)来进行处理。\n",
    "\n",
    "此外，[`torch.permute`](https://pytorch.org/docs/stable/generated/torch.permute.html)可以同时交换多个轴的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:09:08.564403Z",
     "iopub.status.busy": "2024-10-16T01:09:08.564088Z",
     "iopub.status.idle": "2024-10-16T01:09:08.570003Z",
     "shell.execute_reply": "2024-10-16T01:09:08.569564Z",
     "shell.execute_reply.started": "2024-10-16T01:09:08.564384Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "shape: torch.Size([2, 3, 4])\n",
      "\n",
      "Swap axes 1 and 2:\n",
      "tensor([[[ 1,  5,  9],\n",
      "         [ 2,  6, 10],\n",
      "         [ 3,  7, 11],\n",
      "         [ 4,  8, 12]],\n",
      "\n",
      "        [[13, 17, 21],\n",
      "         [14, 18, 22],\n",
      "         [15, 19, 23],\n",
      "         [16, 20, 24]]])\n",
      "torch.Size([2, 4, 3])\n",
      "\n",
      "Permute axes\n",
      "tensor([[[ 1, 13],\n",
      "         [ 2, 14],\n",
      "         [ 3, 15],\n",
      "         [ 4, 16]],\n",
      "\n",
      "        [[ 5, 17],\n",
      "         [ 6, 18],\n",
      "         [ 7, 19],\n",
      "         [ 8, 20]],\n",
      "\n",
      "        [[ 9, 21],\n",
      "         [10, 22],\n",
      "         [11, 23],\n",
      "         [12, 24]]])\n",
      "shape: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of shape (2, 3, 4)\n",
    "x0 = torch.tensor([\n",
    "     [[1,  2,  3,  4],\n",
    "      [5,  6,  7,  8],\n",
    "      [9, 10, 11, 12]],\n",
    "     [[13, 14, 15, 16],\n",
    "      [17, 18, 19, 20],\n",
    "      [21, 22, 23, 24]]])\n",
    "print('Original tensor:')\n",
    "print(x0)\n",
    "print('shape:', x0.shape)\n",
    "\n",
    "# Swap axes 1 and 2; shape is (2, 4, 3)\n",
    "x1 = x0.transpose(1, 2)\n",
    "print('\\nSwap axes 1 and 2:')\n",
    "print(x1)\n",
    "print(x1.shape)\n",
    "\n",
    "# Permute axes; the argument (1, 2, 0) means:\n",
    "# - Make the old dimension 1 appear at dimension 0;\n",
    "# - Make the old dimension 2 appear at dimension 1;\n",
    "# - Make the old dimension 0 appear at dimension 2\n",
    "# This results in a tensor of shape (3, 4, 2)\n",
    "x2 = x0.permute(1, 2, 0)\n",
    "print('\\nPermute axes')\n",
    "print(x2)\n",
    "print('shape:', x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor运算\n",
    "到目前为止，我们已经看到了如何构建、访问和处理张量形状。但使用张量的最重要原因之一是为了计算。PyTorch 提供了许多不同的操作来对张量执行计算。\n",
    "\n",
    "#### 元素级（Elementwise）运算\n",
    "如果我们在两个张量上用基本数学符号，那么会逐元素进行运算。此外，PyTorch提供了sin、cos、sqrt等多种[数学运算](https://pytorch.org/docs/stable/torch.html#pointwise-ops)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:09:43.759764Z",
     "iopub.status.busy": "2024-10-16T01:09:43.759203Z",
     "iopub.status.idle": "2024-10-16T01:09:43.769755Z",
     "shell.execute_reply": "2024-10-16T01:09:43.769215Z",
     "shell.execute_reply.started": "2024-10-16T01:09:43.759744Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementwise sum:\n",
      "tensor([[ 6.,  8., 10., 12.]])\n",
      "tensor([[ 6.,  8., 10., 12.]])\n",
      "tensor([[ 6.,  8., 10., 12.]])\n",
      "\n",
      "Elementwise difference:\n",
      "tensor([[-4., -4., -4., -4.]])\n",
      "tensor([[-4., -4., -4., -4.]])\n",
      "tensor([[-4., -4., -4., -4.]])\n",
      "\n",
      "Elementwise product:\n",
      "tensor([[ 5., 12., 21., 32.]])\n",
      "tensor([[ 5., 12., 21., 32.]])\n",
      "tensor([[ 5., 12., 21., 32.]])\n",
      "\n",
      "Elementwise division\n",
      "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
      "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
      "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
      "\n",
      "Elementwise power\n",
      "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
      "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
      "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
      "\n",
      "Square root:\n",
      "tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n",
      "tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n",
      "\n",
      "Trig functions:\n",
      "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n",
      "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n",
      "tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n",
      "tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
    "y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n",
    "\n",
    "# Elementwise sum; all give the same result\n",
    "print('Elementwise sum:')\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))\n",
    "\n",
    "# Elementwise difference\n",
    "print('\\nElementwise difference:')\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "print(x.sub(y))\n",
    "\n",
    "# Elementwise product\n",
    "print('\\nElementwise product:')\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "print(x.mul(y))\n",
    "\n",
    "# Elementwise division\n",
    "print('\\nElementwise division')\n",
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "print(x.div(y))\n",
    "\n",
    "# Elementwise power\n",
    "print('\\nElementwise power')\n",
    "print(x ** y)\n",
    "print(torch.pow(x, y))\n",
    "print(x.pow(y))\n",
    "\n",
    "print('\\nSquare root:')\n",
    "print(torch.sqrt(x))\n",
    "print(x.sqrt())\n",
    "\n",
    "print('\\nTrig functions:')\n",
    "print(torch.sin(x))\n",
    "print(x.sin())\n",
    "print(torch.cos(x))\n",
    "print(x.cos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 聚合（Reduction）运算\n",
    "\n",
    "到目前为止，我们已经看到了在张量上执行逐元素运算。有时，我们可能希望执行在张量的部分或全部上进行聚合的操作，例如对多个元素求和、求平均、求最大最小值。\n",
    "\n",
    "最简单的缩减操作是求和。我们可以使用[.sum()](https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html) 方法来对整个张量进行缩减，或者使用 dim 参数沿着张量的某个维度进行缩减："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T07:27:36.685947Z",
     "iopub.status.busy": "2024-10-18T07:27:36.685574Z",
     "iopub.status.idle": "2024-10-18T07:27:36.692793Z",
     "shell.execute_reply": "2024-10-18T07:27:36.692280Z",
     "shell.execute_reply.started": "2024-10-18T07:27:36.685925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Sum over entire tensor:\n",
      "tensor(21.)\n",
      "tensor(21.)\n",
      "\n",
      "Sum over the first dimension:\n",
      "tensor([5., 7., 9.])\n",
      "tensor([5., 7., 9.])\n",
      "\n",
      "Sum over the second dimension:\n",
      "tensor([ 6., 15.])\n",
      "tensor([ 6., 15.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]], dtype=torch.float32)\n",
    "print('Original tensor:')\n",
    "print(x)\n",
    "\n",
    "print('\\nSum over entire tensor:')\n",
    "print(torch.sum(x))\n",
    "print(x.sum())\n",
    "\n",
    "# We can sum over the first dimension:\n",
    "print('\\nSum over the first dimension:')\n",
    "print(torch.sum(x, dim=0))\n",
    "print(x.sum(dim=0))\n",
    "\n",
    "# Sum over the second dimension:\n",
    "print('\\nSum over the second dimension:')\n",
    "print(torch.sum(x, dim=1))\n",
    "print(x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某一个维度（dim）的求和，会将这一个维度从原本的张量中去除。如果我们将 .sum() 方法中的keepdim参数设置为True，那么这个维度会被保留。维度大小变为1，其实也就相当于去除了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:02:56.237346Z",
     "iopub.status.busy": "2024-10-18T08:02:56.236721Z",
     "iopub.status.idle": "2024-10-18T08:02:56.243006Z",
     "shell.execute_reply": "2024-10-18T08:02:56.242385Z",
     "shell.execute_reply.started": "2024-10-18T08:02:56.237319Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([3, 4, 5, 6])\n",
      "x.sum(dim=0).shape:  torch.Size([4, 5, 6])\n",
      "x.sum(dim=0, keepdim=True).shape:  torch.Size([1, 4, 5, 6])\n",
      "x.sum(dim=1).shape:  torch.Size([3, 5, 6])\n",
      "x.sum(dim=1, keepdim=True).shape:  torch.Size([3, 1, 5, 6])\n",
      "x.sum(dim=2).shape:  torch.Size([3, 4, 6])\n",
      "x.sum(dim=2, keepdim=True).shape:  torch.Size([3, 4, 1, 6])\n",
      "x.sum(dim=3).shape:  torch.Size([3, 4, 5])\n",
      "x.sum(dim=3, keepdim=True).shape:  torch.Size([3, 4, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of shape (3, 4, 5, 6)\n",
    "x = torch.randn(3, 4, 5, 6)\n",
    "print('x.shape: ', x.shape)\n",
    "\n",
    "# Summing over dim=0 eliminates the dimension at index 0 (of size 3):\n",
    "print('x.sum(dim=0).shape: ', x.sum(dim=0).shape)\n",
    "print('x.sum(dim=0, keepdim=True).shape: ', x.sum(dim=0, keepdim=True).shape)\n",
    "\n",
    "# Summing with dim=1 eliminates the dimension at index 1 (of size 4):\n",
    "print('x.sum(dim=1).shape: ', x.sum(dim=1).shape)\n",
    "print('x.sum(dim=1, keepdim=True).shape: ', x.sum(dim=1, keepdim=True).shape)\n",
    "\n",
    "# Summing with dim=2 eliminates the dimension at index 2 (of size 5):\n",
    "print('x.sum(dim=2).shape: ', x.sum(dim=2).shape)\n",
    "print('x.sum(dim=2, keepdim=True).shape: ', x.sum(dim=2, keepdim=True).shape)\n",
    "\n",
    "# Summing with dim=3 eliminates the dimension at index 3 (of size 6):\n",
    "print('x.sum(dim=3).shape: ', x.sum(dim=3).shape)\n",
    "print('x.sum(dim=3, keepdim=True).shape: ', x.sum(dim=3, keepdim=True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求平均方法 [.mean()](https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean) 和求和的使用方法类似，我们接着简要介绍最值计算方式，以最小值 [.min()](https://pytorch.org/docs/stable/generated/torch.min.html#torch.min) 为例。我们既可以算出整个张量的最小值，也可以算沿着某一维度的最小值，和 .sum()的过程十分类似。需要注意的是， .min() 与 .max() 方法会返回两个结果，一个是最值，还有一个是最值在当前维度的数组下标。这个下标也可以通过 [.argmin()](https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin) 与 [.argmax()](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax) 方法直接算出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:43:46.013740Z",
     "iopub.status.busy": "2024-10-18T11:43:46.013417Z",
     "iopub.status.idle": "2024-10-18T11:43:46.020204Z",
     "shell.execute_reply": "2024-10-18T11:43:46.019600Z",
     "shell.execute_reply.started": "2024-10-18T11:43:46.013722Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[2., 4., 3., 5.],\n",
      "        [3., 3., 5., 2.]]) torch.Size([2, 4])\n",
      "\n",
      "Overall minimum:  tensor(2.)\n",
      "\n",
      "Minimum along each column:\n",
      "values: tensor([2., 3., 3., 2.])\n",
      "idxs: tensor([0, 1, 0, 1])\n",
      "\n",
      "Minimum along each row:\n",
      "values: tensor([2., 2.])\n",
      "idxs: tensor([0, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2, 4, 3, 5], [3, 3, 5, 2]], dtype=torch.float32)\n",
    "print('Original tensor:')\n",
    "print(x, x.shape)\n",
    "\n",
    "# Finding the overall minimum only returns a single value\n",
    "print('\\nOverall minimum: ', x.min())\n",
    "\n",
    "# Compute the minimum along each column; we get both the value and location:\n",
    "# The minimum of the first column is 2, and it appears at index 0;\n",
    "# the minimum of the second column is 3 and it appears at index 1; etc\n",
    "col_min_vals, col_min_idxs = x.min(dim=0)\n",
    "print('\\nMinimum along each column:')\n",
    "print('values:', col_min_vals)\n",
    "print('idxs:', col_min_idxs)\n",
    "\n",
    "# Compute the minimum along each row; we get both the value and the minimum\n",
    "row_min_vals, row_min_idxs = x.min(dim=1)\n",
    "print('\\nMinimum along each row:')\n",
    "print('values:', row_min_vals)\n",
    "print('idxs:', row_min_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵运算\n",
    "与MATLAB不同，* 在PyTorch中表示逐元素乘法，而不是矩阵乘法。PyTorch提供了许多方法来计算不同类型的向量和矩阵积。其中最常用的有：\n",
    "+ [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html): 计算向量的内积\n",
    "+ [`torch.mm`](https://pytorch.org/docs/stable/generated/torch.mm.html): 计算矩阵-矩阵乘积\n",
    "+ [`torch.mv`](https://pytorch.org/docs/stable/generated/torch.mv.html): 计算矩阵-向量乘积\n",
    "所有这些函数也作为张量实例方法提供，例如 Tensor.dot 而不是 torch.dot。\n",
    "\n",
    "以下是使用 torch.dot 计算内积的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T09:14:50.964038Z",
     "iopub.status.busy": "2024-10-19T09:14:50.963730Z",
     "iopub.status.idle": "2024-10-19T09:14:51.003609Z",
     "shell.execute_reply": "2024-10-19T09:14:51.003087Z",
     "shell.execute_reply.started": "2024-10-19T09:14:50.964022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot products:\n",
      "tensor(219.)\n",
      "tensor(219.)\n",
      "\n",
      "1D tensors expected, but got 2D and 2D tensors\n",
      "\n",
      "Matrix-matrix product:\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([9,10], dtype=torch.float32)\n",
    "w = torch.tensor([11, 12], dtype=torch.float32)\n",
    "\n",
    "# Inner product of vectors\n",
    "print('Dot products:')\n",
    "print(torch.dot(v, w))\n",
    "print(v.dot(w))\n",
    "print()\n",
    "\n",
    "# dot only works for vectors -- it will give an error for tensors of rank > 1\n",
    "x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "y = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n",
    "try:\n",
    "  print(x.dot(y))\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "  \n",
    "# Instead we use mm for matrix-matrix products:\n",
    "print('\\nMatrix-matrix product:')\n",
    "print(torch.mm(x, y))\n",
    "print(x.mm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运算中的数组广播（Broadcasting）\n",
    "\n",
    "类似NumPy，PyTorch提供了数组广播的机制。广播允许PyTorch在执行算术操作时使用不同形状的数组。通常我们有一个较小的张量和一个较大的张量，我们希望多次使用较小的张量对较大的张量执行某些操作。\n",
    "\n",
    "例如，假设我们想要将一个常数向量添加到张量的每一行。我们可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T12:14:56.187567Z",
     "iopub.status.busy": "2024-10-18T12:14:56.187269Z",
     "iopub.status.idle": "2024-10-18T12:14:56.191810Z",
     "shell.execute_reply": "2024-10-18T12:14:56.191378Z",
     "shell.execute_reply.started": "2024-10-18T12:14:56.187550Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2,  4],\n",
      "        [ 5,  5,  7],\n",
      "        [ 8,  8, 10],\n",
      "        [11, 11, 13]])\n"
     ]
    }
   ],
   "source": [
    "# We will add the vector v to each row of the matrix x,\n",
    "# storing the result in the matrix y\n",
    "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "y = torch.zeros_like(x)   # Create an empty matrix with the same shape as x\n",
    "\n",
    "# Add the vector v to each row of the matrix x with an explicit loop\n",
    "for i in range(4):\n",
    "    y[i, :] = x[i, :] + v\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法是有效的；然而，当张量 x 非常大时，在 Python 中计算显式循环可能会很慢。请注意，将向量 v 添加到张量 x 的每一行等同于通过垂直堆叠 v 的多个副本形成张量 vv，然后对 x 和 vv 进行逐元素求和。我们可以这样实现这个方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-18T12:20:33.301695Z",
     "iopub.status.busy": "2024-10-18T12:20:33.301401Z",
     "iopub.status.idle": "2024-10-18T12:20:33.305570Z",
     "shell.execute_reply": "2024-10-18T12:20:33.305150Z",
     "shell.execute_reply.started": "2024-10-18T12:20:33.301678Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2,  4],\n",
      "        [ 5,  5,  7],\n",
      "        [ 8,  8, 10],\n",
      "        [11, 11, 13]])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1, 0, 1])\n",
    "vv = v.repeat((4, 1))  # Stack 4 copies of v on top of each other\n",
    "y = x + vv  # Add x and vv elementwise\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样也是有效的；但是，这个做法中 vv 张量相较于前一种方法占据了4倍的内存，在我们使用GPU加速时，显存是非常珍贵的，因此这也不是一个好方法。正确的做法是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T09:14:46.395978Z",
     "iopub.status.busy": "2024-10-19T09:14:46.395491Z",
     "iopub.status.idle": "2024-10-19T09:14:46.400228Z",
     "shell.execute_reply": "2024-10-19T09:14:46.399780Z",
     "shell.execute_reply.started": "2024-10-19T09:14:46.395960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2,  4],\n",
      "        [ 5,  5,  7],\n",
      "        [ 8,  8, 10],\n",
      "        [11, 11, 13]])\n"
     ]
    }
   ],
   "source": [
    "# We will add the vector v to each row of the matrix x,\n",
    "# storing the result in the matrix y\n",
    "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "y = x + v  # Add v to each row of x using broadcasting\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"y = x + v\" 这一行之所以能够工作，尽管 x 的形状是 (4, 3)，而 v 的形状是 (3,)，是因为PyTorch在计算过程中对 v 使用了广播。我们可以认为在形状上，v: (3, ) -> (1, 3) -> (4, 3)；即 v 这个向量被重复了四份和 x 的第一个维度匹配，但是这个重复过程是隐式的，而不是像 .repeat() 方法那样会显式占据内存。广播遵循以下规则：\n",
    "\n",
    "1. 只有在所有维度上都是兼容的情况下，这两个张量才能进行广播。\n",
    "2. 如果两个张量在某一维度上的大小相同，或者其中一个张量在该维度上的大小为 1，则认为它们在该维度上是兼容的。\n",
    "3. 如果张量的秩不同，会默认在较低秩数组的形状前面添加 1，直到两个形状的长度相同。\n",
    "4. 在任何一个维度上，其中一个张量的大小为 1，而另一个张量的大小大于 1，那么前一个张量在该维度上的行为就好像它被沿着那个维度复制。\n",
    "\n",
    "同学可以阅读[文档](https://pytorch.org/docs/stable/notes/broadcasting.html)以获取更详细的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运用GPU处理张量\n",
    "\n",
    "我们先打开Kaggle的GPU环境——点击左上角Settings->Accelerator，选择GPU T4 x2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T09:14:40.165436Z",
     "iopub.status.busy": "2024-10-19T09:14:40.165109Z",
     "iopub.status.idle": "2024-10-19T09:14:40.217194Z",
     "shell.execute_reply": "2024-10-19T09:14:40.216725Z",
     "shell.execute_reply.started": "2024-10-19T09:14:40.165419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch can use GPUs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print('PyTorch can use GPUs!')\n",
    "else:\n",
    "  print('PyTorch cannot use GPUs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要利用GPU加速张量的运算，那么要把张量从CPU迁移到GPU上，使用 .to() 或者 .cuda() 方法。我们也可以直接在GPU上创建张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T12:42:08.367215Z",
     "iopub.status.busy": "2024-10-18T12:42:08.366876Z",
     "iopub.status.idle": "2024-10-18T12:42:08.372577Z",
     "shell.execute_reply": "2024-10-18T12:42:08.372146Z",
     "shell.execute_reply.started": "2024-10-18T12:42:08.367196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 device: cpu\n",
      "x1 device: cuda:0\n",
      "x2 device: cuda:0\n",
      "x3 device: cpu\n",
      "x4 device: cpu\n",
      "y device / dtype: cuda:0 torch.float64\n",
      "x5 device / dtype: cuda:0 torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Construct a tensor on the CPU\n",
    "x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "print('x0 device:', x0.device)\n",
    "\n",
    "# Move it to the GPU using .to()\n",
    "x1 = x0.to('cuda')\n",
    "print('x1 device:', x1.device)\n",
    "\n",
    "# Move it to the GPU using .cuda()\n",
    "x2 = x0.cuda()\n",
    "print('x2 device:', x2.device)\n",
    "\n",
    "# Move it back to the CPU using .to()\n",
    "x3 = x1.to('cpu')\n",
    "print('x3 device:', x3.device)\n",
    "\n",
    "# Move it back to the CPU using .cpu()\n",
    "x4 = x2.cpu()\n",
    "print('x4 device:', x4.device)\n",
    "\n",
    "# We can construct tensors directly on the GPU as well\n",
    "y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n",
    "print('y device / dtype:', y.device, y.dtype)\n",
    "\n",
    "# Calling x.to(y) where y is a tensor will return a copy of x with the same\n",
    "# device and dtype as y\n",
    "x5 = x0.to(y)\n",
    "print('x5 device / dtype:', x5.device, x5.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuda:0与cuda:1表示当前机器的第0号和1号GPU，在没有显式声明的情况下，默认将张量迁移到0号GPU上。\n",
    "\n",
    "如果两个张量在不同的设备上（一个CPU、一个cuda:0或一个cuda:0、一个cuda:1），那么把拿这两个张量做运算会报错。此时我们需要使用 .to() 方法将它们迁移到同一个设备上。\n",
    "\n",
    "以上就是教程的全部，接下来请同学看练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 练习\n",
    "请编写代码使程序能够运行并且通过测试用例。\n",
    "\n",
    "将你完成的练习全部运行一遍后，左上角File->Download Notebook。将下载的ipynb文件按照Lab文档要求进行处理，上传到elearning。该部分完成后还有第二部分https://www.kaggle.com/code/donquijote7/pytorch-logistic-regression\n",
    "\n",
    "如果代码中使用了 for 去遍历张量的某个维度，那么这段代码会扣分。Python中的 for 是非常低效的，使用PyTorch张量的目的就是提高计算效率。在大多数情况下，PyTorch提供的接口能让我们避免 for 的使用。\n",
    "\n",
    "总共四题，每一题25分。\n",
    "\n",
    "如果遇到困难，可以：\n",
    "+ 看看前面的基础知识\n",
    "+ 看看提示\n",
    "+ 点击提示中给出的pytorch函数链接，去官方文档看看这个函数是怎么使用的。感兴趣的同学可以看看[pytorch官网总文档](https://pytorch.org/docs/stable/index.html)，里面内容较多。\n",
    "+ 鼓励大家助教群中提问交流，不要抄袭同学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建Tensor\n",
    "在函数`multiples_of_ten`中创建一个tensor，数据类型是`torch.float64`，包含了从start到stop之间的10的倍数。\n",
    "\n",
    "提示：[`torch.arange`](https://pytorch.org/docs/stable/generated/torch.arange.html) [`torch.empty`](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-19T06:18:07.676801Z",
     "iopub.status.busy": "2024-10-19T06:18:07.676472Z",
     "iopub.status.idle": "2024-10-19T06:18:07.682428Z",
     "shell.execute_reply": "2024-10-19T06:18:07.681963Z",
     "shell.execute_reply.started": "2024-10-19T06:18:07.676783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct dtype:  True\n",
      "Correct shape:  True\n",
      "Correct values:  True\n",
      "\n",
      "Correct dtype:  True\n",
      "Correct shape:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def multiples_of_ten(start: int, stop: int) -> torch.Tensor:\n",
    "    assert start <= stop\n",
    "    x = None\n",
    "    x=torch.arange(start,stop+1,dtype=torch.float64)\n",
    "    if (x%10==0).any():\n",
    "        x1=x[x%10==0]\n",
    "        return x1\n",
    "    else:\n",
    "        x=torch.empty((0,),dtype=torch.float64)\n",
    "        return x\n",
    "    \n",
    "start = 5\n",
    "stop = 25\n",
    "x = multiples_of_ten(start, stop)\n",
    "#print(x)\n",
    "print('Correct dtype: ', x.dtype == torch.float64)\n",
    "print('Correct shape: ', x.shape == (2,))\n",
    "print('Correct values: ', x.tolist() == [10, 20])\n",
    "\n",
    "# If there are no multiples of ten in the given range you should return an empty tensor\n",
    "start = 5\n",
    "stop = 7\n",
    "x = multiples_of_ten(start, stop)\n",
    "#print(x)\n",
    "print('\\nCorrect dtype: ', x.dtype == torch.float64)\n",
    "print('Correct shape: ', x.shape == (0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切片、形状操作\n",
    "实现函数`reshape_practice`使输出和`expected`相同。\n",
    "\n",
    "提示：使用 .view() 以及[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html)。这题有一定难度，同学可以先观察y中连续的部分，使用 .view() 将x转化为类似的形状。随后使用切片的方式切出两个部分。最后把两个部分利用cat函数拼接。\n",
    "\n",
    "在这个过程中同学可以输出一些中间值观察一下，便于理解和debug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-19T07:43:44.033702Z",
     "iopub.status.busy": "2024-10-19T07:43:44.033384Z",
     "iopub.status.idle": "2024-10-19T07:43:44.040731Z",
     "shell.execute_reply": "2024-10-19T07:43:44.040246Z",
     "shell.execute_reply.started": "2024-10-19T07:43:44.033685Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is x:\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "Here is y:\n",
      "tensor([[ 0,  2,  1,  3, 12, 13, 14, 15],\n",
      "        [ 4,  6,  5,  7, 16, 17, 18, 19],\n",
      "        [ 8, 10,  9, 11, 20, 21, 22, 23]])\n",
      "Correct: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_practice(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = None\n",
    "    x1=x[12:24]\n",
    "    x2=x1.view(3,4)\n",
    "    #print(x2)\n",
    "    z1=x[0:12]\n",
    "    #print(z1)\n",
    "    z2=z1.view(3,4)\n",
    "    #print(z2)\n",
    "    idx=[0,2,1,3]\n",
    "    z3=z2[:,idx]\n",
    "    #print(z2[:,idx])\n",
    "    y=torch.cat((z3,x2),dim=1)\n",
    "    #print(y)\n",
    "    return y\n",
    "\n",
    "x = torch.arange(24)\n",
    "print('Here is x:')\n",
    "print(x)\n",
    "y = reshape_practice(x)\n",
    "print('Here is y:')\n",
    "print(y)\n",
    "\n",
    "expected = [\n",
    "    [0,  2,  1,  3, 12, 13, 14, 15],\n",
    "    [4,  6,  5,  7, 16, 17, 18, 19],\n",
    "    [8, 10,  9, 11, 20, 21, 22, 23]]\n",
    "print('Correct:', y.tolist() == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚合运算\n",
    "实现函数`normalize_columns`，该函数对矩阵的列做normalize操作。\n",
    "\n",
    "```\n",
    "x = [[ 0,  30,  600],\n",
    "     [ 1,  10,  200],\n",
    "     [-1,  20,  400]]\n",
    "```\n",
    "- 第一列均值 0、 标准差 1\n",
    "- 第二列均值 20、 标准差 10\n",
    "- 第三列均值 400、 标准差 200\n",
    "\n",
    "```\n",
    "y = [[ 0,  1,  1],\n",
    "     [ 1, -1, -1],\n",
    "     [-1,  0,  0]]\n",
    "```\n",
    "\n",
    "如果某一列为 $x_1,\\ldots,x_M$、均值 $\\mu$、标准差 $\\sigma$，则\n",
    "\n",
    "$$\\mu=\\frac{1}{M}\\sum_{i=1}^M x_i \\hspace{4pc} \\sigma = \\sqrt{\\frac{1}{M-1}\\sum_{i=1}^M(x_i-\\mu)^2}$$\n",
    "\n",
    "归一化后的结果为$$y_i=\\frac{x_i-\\mu}{\\sigma}$$\n",
    "\n",
    "注意：只允许使用`+, -, *, /, **, sqrt, sum`等基础操作，不允许使用`torch.mean`和`torch.std`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-19T08:35:20.006001Z",
     "iopub.status.busy": "2024-10-19T08:35:20.005449Z",
     "iopub.status.idle": "2024-10-19T08:35:20.012128Z",
     "shell.execute_reply": "2024-10-19T08:35:20.011669Z",
     "shell.execute_reply.started": "2024-10-19T08:35:20.005983Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is x:\n",
      "tensor([[  0.,  30., 600.],\n",
      "        [  1.,  10., 200.],\n",
      "        [ -1.,  20., 400.]])\n",
      "Here is y:\n",
      "tensor([[ 0.,  1.,  1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [-1.,  0.,  0.]])\n",
      "y correct:  True\n",
      "x unchanged:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def normalize_columns(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = None\n",
    "    mean=x.sum(dim=0)/x.size(0)\n",
    "    var=((x-mean)**2).sum(dim=0)/(x.size(0)-1)\n",
    "    std=torch.sqrt(var)\n",
    "    y=(x-mean)/std\n",
    "    return y\n",
    "\n",
    "x = torch.tensor([[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]])\n",
    "y = normalize_columns(x)\n",
    "print('Here is x:')\n",
    "print(x)\n",
    "print('Here is y:')\n",
    "print(y)\n",
    "\n",
    "x_expected = [[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]]\n",
    "y_expected = [[0., 1., 1.], [1., -1., -1.], [-1., 0., 0.]]\n",
    "y_correct = y.tolist() == y_expected\n",
    "x_correct = x.tolist() == x_expected\n",
    "print('y correct: ', y_correct)\n",
    "print('x unchanged: ', x_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚合、索引赋值\n",
    "\n",
    "在不用for循环的情况下，使用reduction和indexing操作，把张量每一行最小的元素设置为0.\n",
    "\n",
    "提示：在这题中，同学需要先找出最小值所在的位置。随后利用这个位置进行索引+赋值操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-19T09:13:10.553075Z",
     "iopub.status.busy": "2024-10-19T09:13:10.552759Z",
     "iopub.status.idle": "2024-10-19T09:13:10.560870Z",
     "shell.execute_reply": "2024-10-19T09:13:10.560364Z",
     "shell.execute_reply.started": "2024-10-19T09:13:10.553058Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is x0:\n",
      "tensor([[10, 20, 30],\n",
      "        [ 2,  5,  1]])\n",
      "Here is y0:\n",
      "tensor([[ 0, 20, 30],\n",
      "        [ 2,  5,  0]])\n",
      "y0 correct:  True\n",
      "\n",
      "Here is x1:\n",
      "tensor([[ 2,  5, 10, -1],\n",
      "        [ 1,  3,  2,  4],\n",
      "        [ 5,  6,  2, 10]])\n",
      "Here is y1:\n",
      "tensor([[ 2,  5, 10,  0],\n",
      "        [ 0,  3,  2,  4],\n",
      "        [ 5,  6,  0, 10]])\n",
      "y1 correct:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def zero_row_min(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = x.clone()\n",
    "    _,idx=x.min(dim=1)\n",
    "    y[torch.arange(x.size(0)),idx]=0 #the locations where the mins on and let them = 0\n",
    "    return y\n",
    "\n",
    "x0 = torch.tensor([[10, 20, 30], [2, 5, 1]])\n",
    "print('Here is x0:')\n",
    "print(x0)\n",
    "y0 = zero_row_min(x0)\n",
    "print('Here is y0:')\n",
    "print(y0)\n",
    "expected = [[0, 20, 30], [2, 5, 0]]\n",
    "y0_correct = torch.is_tensor(y0) and y0.tolist() == expected\n",
    "print('y0 correct: ', y0_correct)\n",
    "\n",
    "x1 = torch.tensor([[2, 5, 10, -1], [1, 3, 2, 4], [5, 6, 2, 10]])\n",
    "print('\\nHere is x1:')\n",
    "print(x1)\n",
    "y1 = zero_row_min(x1)\n",
    "print('Here is y1:')\n",
    "print(y1)\n",
    "expected = [[2, 5, 10, 0], [0, 3, 2, 4], [5, 6, 0, 10]]\n",
    "y1_correct = torch.is_tensor(y1) and y1.tolist() == expected\n",
    "print('y1 correct: ', y1_correct)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
